# -*- coding: utf-8 -*-
"""Pneumonia_MRI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jr0gxdNa0JAUDl1tjTCElaLxsKbffGI8

<img src='https://ipmcmed.com/wp-content/uploads/2013/02/23977230-nano-mri-diamonds.jpg' height='400' width='850'/>

>Pneumonia, a prevalent respiratory threat globally and particularly in regions like Tanzania, presents a critical need for fast and accurate diagnosis to ensure effective treatment. However, existing diagnostic processes encounter challenges, leading to delays and potential inaccuracies in pneumonia detection. The manual interpretation of medical imaging, such as X-rays, MRI, and CT scans, is resource-intensive and susceptible to human error.

## Group Members
| S/N  |STUDENT NAME   |REGISTRATION NUMBER|POGRAM   |   |
|---|---|---|---|---|
|  1 |  SAIDI ALLY ATHUMANI		 | T21-03-03113  |  BSC-HIS |   |
|  2 |DAVID DAUSON RUTALOMBA   | T21-03-12925  | BSC-HIS  |   |
|  3 | BARAKA GODFREY GISHA  | T21-03-05608  | BSC-HIS  |   |
|4|PATRICK RICHARD|T21-03-13227|BSC-HIS| |
|5|NEEMA JOSHUA|T21-03-10273|BSC-HIS||
"""

# Commented out IPython magic to ensure Python compatibility.
# Pytorch libraries
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision import datasets, transforms, models
from torch.optim.lr_scheduler import StepLR
from torchsummary import summary

# Utils libraries
import os
from tqdm import tqdm
import copy
import random
import warnings
from timeit import default_timer as timer

# Imaging and graphing libraries
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.image as mpimg

# Data manipulation libraries
import numpy as np
import pandas as pd

# %matplotlib inline
warnings.filterwarnings('ignore', category=FutureWarning)

"""### Setting file paths"""

def get_files():
  data_dir = './drive/MyDrive/pneumonia_MRI'
  train_dir = os.path.join(data_dir, 'Train')
  test_dir = os.path.join(data_dir, 'Test')

  return train_dir, test_dir, data_dir

from google.colab import drive
drive.mount('/content/drive')

"""##### Global Variables"""

BATCH_SIZE = 4
NUM_EPOCHS = 10
LEARNING_RATE = 0.01
NUM_THREADS = 2

train_dir, test_dir, path = get_files()

print(f'There are {len(os.listdir(train_dir))} classes in a dataset: {os.listdir(train_dir)}')

"""##### Data Exploration"""

def display_mri_images():
  files = []
  cats = []
  filenames = os.listdir(os.path.join(train_dir, 'NORMAL'))
  for file in filenames:
    files.append(os.path.join(train_dir, 'NORMAL', file))
    cats.append('NORMAL')

  filenames = os.listdir(os.path.join(train_dir, 'PNEUMONIA'))
  for file in filenames:
    files.append(os.path.join(train_dir, 'PNEUMONIA', file))
    cats.append('PNEUMONIA')

  img_count = 4
  file_idx = random.sample(range(len(files)), img_count)
  rand_fig = plt.figure(figsize=(12, 5))
  for idx in range(img_count):
    rand_fig.add_subplot(1, 4, idx + 1)
    plt.imshow(cv2.imread(files[file_idx[idx]]))
    plt.title(cats[file_idx[idx]])
    plt.axis('off')
  plt.show()

display_mri_images()

def train_mri_graph():
  normal_paths = [
      f'{train_dir}/NORMAL',
      f'{test_dir}/NORMAL',
  ]
  pneumonia_paths = [
      f'{train_dir}/PNEUMONIA',
      f'{test_dir}/PNEUMONIA',
  ]
  data_dirs = [normal_paths, pneumonia_paths]
  filepaths = []
  labels = []
  classes = ['Normal', 'Pneumonia']
  #
  for idx, paths in enumerate(data_dirs):
    for path in paths:
      file_lst = os.listdir(path)
      for f in file_lst:
        fpath = os.path.join(path, f)
        filepaths.append(fpath)
        labels.append(classes[idx])

  file_series = pd.Series(filepaths, name='filepaths')
  lable_series = pd.Series(labels, name='category')
  data = pd.concat([file_series, lable_series], axis =1 )
  df = pd.DataFrame(data)

  plt.figure(figsize = (12, 4))
  sns.histplot(data=df, x='category', hue='category')
  plt.title('Patients per condition');


train_mri_graph()

"""#### Data Preparation"""

def prepareData():
  train_transform = transforms.Compose([
      transforms.Resize(224),
      transforms.CenterCrop(224),
      transforms.ToTensor(),
      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
  ])
  test_transform = transforms.Compose([
      transforms.Resize(224),
      transforms.CenterCrop(224),
      transforms.ToTensor(),
      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
  ])
  return train_transform, test_transform

def create_dataLoaders():
  train_transform, test_transform = prepareData()
  #
  train_data = datasets.ImageFolder(train_dir, transform=train_transform)
  test_data = datasets.ImageFolder(test_dir, transform=test_transform)

  print(f'Number of train data: {len(train_data)}')
  print(f'Number of test data: {len(test_data)}')

  train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
  test_loader = DataLoader(test_data, batch_size=1, shuffle=False)

  return train_loader, test_loader

train_loader, test_loader = create_dataLoaders()

"""#### Pneumonia MRI Model"""

class PneumoniaMriModel(nn.Module):
  def __init__(self):
    super(PneumoniaMriModel, self).__init__()
    # Input Block
    self.convblock1 = nn.Sequential(
    nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(8)
    )
    self.pool11 = nn.MaxPool2d(2, 2)
    # CONVOLUTION BLOCK 1
    self.convblock2 = nn.Sequential(
    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(16)
    )
    self.pool22 = nn.MaxPool2d(2, 2)
    self.convblock3 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(10),
    )
    self.pool33 = nn.MaxPool2d(2, 2)
    # CONVOLUTION BLOCK 2
    self.convblock4 = nn.Sequential(
    nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(10)
    )
    self.convblock5 = nn.Sequential(
    nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(32),
    )
    self.convblock6 = nn.Sequential(
    nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(10),
    )
    self.convblock7 = nn.Sequential(
    nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(10)
    )
    self.convblock8 = nn.Sequential(
    nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(32)
    )
    self.convblock9 = nn.Sequential(
    nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(10)
    )
    self.convblock10 = nn.Sequential(
    nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(14)
    )
    self.convblock11 = nn.Sequential(
    nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),
    nn.ReLU(),
    nn.BatchNorm2d(16)
    )
    # OUTPUT BLOCK
    self.gap = nn.Sequential(
    nn.AvgPool2d(kernel_size=4)
    )
    self.convblockout = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(4, 4), padding=0, bias=False),)

  def forward(self, x):
    x = self.convblock1(x)
    x = self.pool11(x)
    x = self.convblock2(x)
    x = self.pool22(x)
    x = self.convblock3(x)
    x = self.pool33(x)
    x = self.convblock4(x)
    x = self.convblock5(x)
    x = self.convblock6(x)
    x = self.convblock7(x)
    x = self.convblock8(x)
    x = self.convblock9(x)
    x = self.convblock10(x)
    x = self.convblock11(x)
    x = self.gap(x)
    x = self.convblockout(x)
    x = x.view(-1, 2)

    return F.log_softmax(x, dim=-1)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print("Available processor {}".format(device))
model = PneumoniaMriModel().to(device)
summary(model, input_size=(3, 224, 224))

"""##### Train the model"""

train_losses = []
test_losses = []
train_acc = []
test_acc = []

def train(model, device, train_loader, optimizer, epoch):
 model.train()
 pbar = tqdm(train_loader)
 correct = 0
 processed = 0
 for batch_idx, (data, target) in enumerate(pbar):
  # get data
  data, target = data.to(device), target.to(device)
  # Initialization of gradient
  optimizer.zero_grad()

  ## prediction on data
  y_pred = model(data)
  # Calculating loss given the prediction
  loss = F.nll_loss(y_pred, target)
  train_losses.append(loss)
  # Backprop
  loss.backward()
  optimizer.step()

  pred = y_pred.argmax(dim=1, keepdim=True)
  correct += pred.eq(target.view_as(pred)).sum().item()
  processed += len(data)

  pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')
  train_acc.append(100*correct/processed)

"""##### Test the model"""

def test_model(model, device, test_loader):
  model.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad():
    for data, target in test_loader:
      data, target = data.to(device), target.to(device)
      output = model(data)
      test_loss += F.nll_loss(output, target, reduction='sum').item()
      pred = output.argmax(dim=1, keepdim=True)
      correct += pred.eq(target.view_as(pred)).sum().item()
  test_loss /= len(test_loader.dataset)
  test_losses.append(test_loss)
  print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(test_loss, correct, len(test_loader.dataset),
                                                                               100. * correct / len(test_loader.dataset)))
  test_acc.append(100. * correct / len(test_loader.dataset))

def run_test():
  #
  model = PneumoniaMriModel().to(device)
  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)
  scheduler = StepLR(optimizer, step_size=6, gamma=0.5)
  #
  for epoch in range(NUM_EPOCHS):
    print("EPOCH:", epoch + 1)
    train(model, device, train_loader, optimizer, epoch)
    scheduler.step()
    print('current Learning Rate: ', optimizer.state_dict()["param_groups"][0]["lr"])
    test_model(model, device, test_loader)

run_test()

def plot_model_stats():
  # Gather data
  train_losses1 = [float(i.cpu().detach().numpy()) for i in train_losses]
  train_acc1 = [i for i in train_acc]
  test_losses1 = [i for i in test_losses]
  test_acc1 = [i for i in test_acc]

  # Plotting model stats
  fig, axs = plt.subplots(2,2,figsize=(16,10))
  axs[0, 0].plot(train_losses1,color='green')
  axs[0, 0].set_title("Training Loss")
  axs[1, 0].plot(train_acc1,color='green')
  axs[1, 0].set_title("Training Accuracy")
  axs[0, 1].plot(test_losses1)
  axs[0, 1].set_title("Test Loss")
  axs[1, 1].plot(test_acc1)
  axs[1, 1].set_title("Test Accuracy")

plot_model_stats()

def get_all_predictions(model, loader):
  all_preds = torch.tensor([])
  for batch in loader:
    images, labels = batch

    preds = model(images)
    all_preds = torch.cat((all_preds, preds), dim=0)
  return all_preds

model = PneumoniaMriModel()

prediction_loader, _ = create_dataLoaders()
train_preds = get_all_predictions(model, prediction_loader)

"""#### Model performance: Confusion Marix"""

with torch.no_grad():
  train_preds = get_all_predictions(model, prediction_loader)

train_transform, _ = prepareData()
train_data = datasets.ImageFolder(train_dir, transform=train_transform)
stacked = torch.stack(
    (torch.Tensor(train_data.targets), train_preds.argmax(dim=1)), dim=1
)

from sklearn.metrics import confusion_matrix

cats = ['Normal', 'Pneumonia']
labels = ['No Neg', 'False Pos', 'False Neg', 'True Pos']
labels = np.asarray(labels).reshape(2, -1)
cm = confusion_matrix(train_data.targets, train_preds.argmax(dim=1))
fig,ax = plt.subplots(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='.4g', xticklabels=['Normal', 'Pneumonia'],yticklabels=['Normal', 'Pneumonia'],ax=ax, cmap='Blues')
plt.title('Confusion matrix: Pneumonia MRI model')
plt.show()

from PIL import Image

def predict_image():
  transform = transforms.Compose([
        transforms.Resize(224),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
  rand_idx = random.randint(0, len(test_loader) -1)
  img_path, label = test_loader.dataset.imgs[rand_idx]
  categories = test_loader.dataset.classes
  image = Image.open(img_path)
  image = transform(image).unsqueeze(0)  # Add batch dimension
  # Set model to evaluation mode
  model.eval()

  # Perform inference
  with torch.no_grad():
      outputs = model(image)

  # Interpret predictions
  _, predicted = torch.max(outputs, 1)
  predicted_class = predicted.item()
  print('Predicted:', categories[predicted_class])
  print('True:', categories[label])
  return Image.open(img_path)

predict_image()

